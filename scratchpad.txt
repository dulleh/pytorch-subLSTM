(base) bash-4.2$ python run.py --verbose --timing --model=subLSTMCuda --cuda --batch-size=3 --seq-length=7 --nhid=2 --epochs=1 --training-size=7
cuda is available: True
cudnn enabled: True
Training subLSTMCuda model with parameters:
	number of layers: 1
	hidden units: 2
	max epochs: 1
	batch size: 3
	optimizer: rmsprop, lr=0.0001, l2=0
	using CUDA
training epoch 1
dE/dh0.01 *
-2.0290  2.7442
 -6.8647  9.2844
  5.5416 -7.4950
[ CUDAFloatType{3,2} ]
grad_cell 0  0
 0  0
 0  0
[ CUDAFloatType{3,2} ]
new_cell-0.0321  0.9393
-0.0242  0.8467
 0.0110  0.8795
[ CUDAFloatType{3,2} ]
input_gate 0.5361  0.2002
 0.5110  0.2517
 0.5107  0.2630
[ CUDAFloatType{3,2} ]
output_gate 0.2663  0.6520
 0.3032  0.6518
 0.3073  0.6554
[ CUDAFloatType{3,2} ]
candidate_cell 0.5946  0.6204
 0.5361  0.6485
 0.5282  0.6551
[ CUDAFloatType{3,2} ]
X-0.0038  0.1337  0.9442  0.0000
 0.1428  0.0621  0.4712  0.0000
 0.1907  0.0474  0.4006  0.0000
[ CUDAFloatType{3,4} ]
gate_weights.sizes()[3, 4, 2]
gate_weights(1,.,.) =
  0.1446 -1.3848
 -1.0133  0.6277
  0.3829  0.4913
 -0.3046  0.5761

(2,.,.) =
  0.0441 -1.0894
 -0.8320  0.6269
  0.1446  0.6122
  0.0256  0.2734

(3,.,.) =
  0.0427 -1.0304
 -0.8125  0.6429
  0.1131  0.6414
  0.0827  0.2280
[ CUDAFloatType{3,4,2} ]
weights.sizes()[8, 4]
weights 0.6010  0.5108  0.3214  0.6770
 0.6453  0.4694 -0.4958  0.2441
-0.2626  0.1918 -0.4936  0.5788
 0.6845  0.4401  0.1471 -0.4400
 0.0896 -0.4237  0.5956 -0.3823
 0.4720  0.2928 -0.1539 -0.2095
 0.2722 -0.1878 -0.5853  0.2427
 0.0263  0.2287  0.6134  0.0776
[ CUDAFloatType{8,4} ]
old_cell-0.2133  0.8109
-0.0973  0.7923
-0.0126  0.8755
[ CUDAFloatType{3,2} ]
dE/dh0.01 *
-0.4512 -0.0665
 -1.6096 -0.2127
  1.3099  0.1582
[ CUDAFloatType{3,2} ]
grad_cell0.01 *
-0.2152  0.3550
 -0.8689  1.1076
  0.7213 -0.8649
[ CUDAFloatType{3,2} ]
new_cell-0.2133  0.8109
-0.0973  0.7923
-0.0126  0.8755
[ CUDAFloatType{3,2} ]
input_gate 0.6736  0.3358
 0.4810  0.2520
 0.5142  0.2649
[ CUDAFloatType{3,2} ]
output_gate 0.4506  0.5586
 0.3329  0.6262
 0.3061  0.6585
[ CUDAFloatType{3,2} ]
candidate_cell 0.4187  0.6187
 0.5013  0.6402
 0.5288  0.6568
[ CUDAFloatType{3,2} ]
X 0.2686  0.0389  0.2838  1.0000
-0.0269  0.1281  0.3099  0.0000
 0.2056  0.0535  0.4066  0.0000
[ CUDAFloatType{3,4} ]
gate_weights.sizes()[3, 4, 2]
gate_weights(1,.,.) =
  0.7246 -0.6821
 -0.1981  0.2353
 -0.3281  0.4841
  0.4166  0.2341

(2,.,.) =
 -0.0761 -1.0879
 -0.6951  0.5161
  0.0054  0.5763
  0.0614  0.1851

(3,.,.) =
  0.0566 -1.0209
 -0.8183  0.6566
  0.1155  0.6493
  0.0821  0.2335
[ CUDAFloatType{3,4,2} ]
weights.sizes()[8, 4]
weights 0.6010  0.5108  0.3214  0.6770
 0.6453  0.4694 -0.4958  0.2441
-0.2626  0.1918 -0.4936  0.5788
 0.6845  0.4401  0.1471 -0.4400
 0.0896 -0.4237  0.5956 -0.3823
 0.4720  0.2928 -0.1539 -0.2095
 0.2722 -0.1878 -0.5853  0.2427
 0.0263  0.2287  0.6134  0.0776
[ CUDAFloatType{8,4} ]
old_cell 0.0691  0.9457
-0.2284  0.7400
-0.0524  0.8663
[ CUDAFloatType{3,2} ]
dE/dh0.001 *
 0.0255  0.8462
  0.5726  2.9131
 -0.3749 -2.4175
[ CUDAFloatType{3,2} ]
grad_cell0.001 *
-1.4540  1.0272
 -4.3766  3.0544
  3.6588 -2.5108
[ CUDAFloatType{3,2} ]
new_cell 0.0691  0.9457
-0.2284  0.7400
-0.0524  0.8663
[ CUDAFloatType{3,2} ]
input_gate 0.5624  0.2186
 0.6595  0.3487
 0.5259  0.2177
[ CUDAFloatType{3,2} ]
output_gate 0.2486  0.6814
 0.4701  0.5489
 0.2813  0.6505
[ CUDAFloatType{3,2} ]
candidate_cell 0.6121  0.6411
 0.3973  0.6213
 0.5710  0.6300
[ CUDAFloatType{3,2} ]
X 0.2517  0.0328  0.9569  0.0000
 0.2466  0.0339  0.1352  1.0000
 0.0327  0.1216  0.7675  0.0000
[ CUDAFloatType{3,4} ]
gate_weights.sizes()[3, 4, 2]
gate_weights(1,.,.) =
  0.2507 -1.2736
 -1.1060  0.7601
  0.4561  0.5803
 -0.2235  0.5676

(2,.,.) =
  0.6611 -0.6250
 -0.1199  0.1962
 -0.4165  0.4951
  0.4985  0.1412

(3,.,.) =
  0.1036 -1.2793
 -0.9379  0.6214
  0.2860  0.5321
 -0.1890  0.4659
[ CUDAFloatType{3,4,2} ]
weights.sizes()[8, 4]
weights 0.6010  0.5108  0.3214  0.6770
 0.6453  0.4694 -0.4958  0.2441
-0.2626  0.1918 -0.4936  0.5788
 0.6845  0.4401  0.1471 -0.4400
 0.0896 -0.4237  0.5956 -0.3823
 0.4720  0.2928 -0.1539 -0.2095
 0.2722 -0.1878 -0.5853  0.2427
 0.0263  0.2287  0.6134  0.0776
[ CUDAFloatType{8,4} ]
old_cell 0.0435  0.8198
 0.0543  0.8730
-0.2154  0.7389
[ CUDAFloatType{3,2} ]
dE/dh0.0001 *
-0.4261  0.8948
 -2.3992  2.4661
  1.2392 -1.4736
[ CUDAFloatType{3,2} ]
grad_cell0.001 *
-0.2842  0.5272
 -1.6059  1.2160
  0.7081 -1.2574
[ CUDAFloatType{3,2} ]
new_cell 0.0435  0.8198
 0.0543  0.8730
-0.2154  0.7389
[ CUDAFloatType{3,2} ]
input_gate 0.5397  0.2102
 0.5429  0.2324
 0.6847  0.2865
[ CUDAFloatType{3,2} ]
output_gate 0.2591  0.6614
 0.2670  0.6715
 0.4136  0.5551
[ CUDAFloatType{3,2} ]
candidate_cell 0.6027  0.6300
 0.5858  0.6453
 0.4691  0.5957
[ CUDAFloatType{3,2} ]
X 0.1382  0.0222  0.9007  0.0000
 0.2367  0.0178  0.7646  0.0000
 0.1880  0.0188  0.6246  1.0000
[ CUDAFloatType{3,4} ]
gate_weights.sizes()[3, 4, 2]
gate_weights(1,.,.) =
  0.1590 -1.3239
 -1.0505  0.6694
  0.4169  0.5323
 -0.2195  0.5277

(2,.,.) =
  0.1722 -1.1950
 -1.0099  0.7148
  0.3465  0.5984
 -0.1122  0.4457

(3,.,.) =
  0.7755 -0.9124
 -0.3490  0.2214
 -0.1239  0.3878
  0.1990  0.4364
[ CUDAFloatType{3,4,2} ]
weights.sizes()[8, 4]
weights 0.6010  0.5108  0.3214  0.6770
 0.6453  0.4694 -0.4958  0.2441
-0.2626  0.1918 -0.4936  0.5788
 0.6845  0.4401  0.1471 -0.4400
 0.0896 -0.4237  0.5956 -0.3823
 0.4720  0.2928 -0.1539 -0.2095
 0.2722 -0.1878 -0.5853  0.2427
 0.0263  0.2287  0.6134  0.0776
[ CUDAFloatType{8,4} ]
old_cell-0.0439  0.6358
 0.0244  0.7547
 0.0004  0.7074
[ CUDAFloatType{3,2} ]
dE/dh0.0001 *
 0.0447  0.3232
  0.5142  1.9308
 -0.0130 -0.9081
[ CUDAFloatType{3,2} ]
grad_cell0.0001 *
-0.6111  2.2049
 -3.8602  4.8316
  2.3070 -4.8347
[ CUDAFloatType{3,2} ]
new_cell-0.0439  0.6358
 0.0244  0.7547
 0.0004  0.7074
[ CUDAFloatType{3,2} ]
input_gate 0.4683  0.2921
 0.5340  0.2251
 0.5020  0.2672
[ CUDAFloatType{3,2} ]
output_gate 0.3508  0.6316
 0.2694  0.6624
 0.3121  0.6510
[ CUDAFloatType{3,2} ]
candidate_cell 0.4732  0.6612
 0.5855  0.6386
 0.5229  0.6560
[ CUDAFloatType{3,2} ]
X 0.1428  0.0150  0.0146  0.0000
 0.1792  0.0131  0.7681  0.0000
 0.2020  0.0079  0.3346  0.0000
[ CUDAFloatType{3,4} ]
gate_weights.sizes()[3, 4, 2]
gate_weights(1,.,.) =
 -0.1268 -0.8851
 -0.6156  0.5390
 -0.1074  0.6687
  0.3017 -0.0175

(2,.,.) =
  0.1364 -1.2361
 -0.9975  0.6740
  0.3455  0.5694
 -0.1291  0.4453

(3,.,.) =
  0.0081 -1.0089
 -0.7905  0.6236
  0.0915  0.6454
  0.1318  0.1788
[ CUDAFloatType{3,4,2} ]
weights.sizes()[8, 4]
weights 0.6010  0.5108  0.3214  0.6770
 0.6453  0.4694 -0.4958  0.2441
-0.2626  0.1918 -0.4936  0.5788
 0.6845  0.4401  0.1471 -0.4400
 0.0896 -0.4237  0.5956 -0.3823
 0.4720  0.2928 -0.1539 -0.2095
 0.2722 -0.1878 -0.5853  0.2427
 0.0263  0.2287  0.6134  0.0776
[ CUDAFloatType{8,4} ]
old_cell-0.0848  0.5382
-0.0580  0.5598
-0.0384  0.5851
[ CUDAFloatType{3,2} ]
dE/dh1e-05 *
-0.3176  0.4200
 -0.4848  2.2609
  0.2587 -2.1352
[ CUDAFloatType{3,2} ]
grad_cell0.0001 *
-0.1955  0.5779
 -0.7845  2.0511
  0.6534 -1.5432
[ CUDAFloatType{3,2} ]
new_cell-0.0848  0.5382
-0.0580  0.5598
-0.0384  0.5851
[ CUDAFloatType{3,2} ]
input_gate 0.4677  0.2495
 0.4881  0.2216
 0.5055  0.2128
[ CUDAFloatType{3,2} ]
output_gate 0.3360  0.6164
 0.3063  0.6233
 0.2884  0.6343
[ CUDAFloatType{3,2} ]
candidate_cell 0.5007  0.6365
 0.5428  0.6240
 0.5664  0.6226
[ CUDAFloatType{3,2} ]
X-0.0382  0.0663  0.2633  0.0000
-0.0594  0.0693  0.5520  0.0000
-0.0192  0.0636  0.7026  0.0000
[ CUDAFloatType{3,4} ]
gate_weights.sizes()[3, 4, 2]
gate_weights(1,.,.) =
 -0.1294 -1.1011
 -0.6810  0.4743
  0.0028  0.5601
  0.0972  0.1421

(2,.,.) =
 -0.0478 -1.2565
 -0.8174  0.5036
  0.1716  0.5065
 -0.0781  0.3193

(3,.,.) =
  0.0218 -1.3079
 -0.9034  0.5507
  0.2673  0.5006
 -0.1542  0.4115
[ CUDAFloatType{3,4,2} ]
weights.sizes()[8, 4]
weights 0.6010  0.5108  0.3214  0.6770
 0.6453  0.4694 -0.4958  0.2441
-0.2626  0.1918 -0.4936  0.5788
 0.6845  0.4401  0.1471 -0.4400
 0.0896 -0.4237  0.5956 -0.3823
 0.4720  0.2928 -0.1539 -0.2095
 0.2722 -0.1878 -0.5853  0.2427
 0.0263  0.2287  0.6134  0.0776
[ CUDAFloatType{8,4} ]
old_cell-0.2246  0.2825
-0.2346  0.2718
-0.2153  0.2915
[ CUDAFloatType{3,2} ]
dE/dh1e-06 *
 0.3951  2.0266
  1.8396  6.8491
 -1.3212 -5.3516
[ CUDAFloatType{3,2} ]
grad_cell1e-05 *
-0.5790  1.7093
 -1.8694  7.1828
  1.4216 -5.8773
[ CUDAFloatType{3,2} ]
new_cell-0.2246  0.2825
-0.2346  0.2718
-0.2153  0.2915
[ CUDAFloatType{3,2} ]
input_gate 0.6228  0.3080
 0.6113  0.3243
 0.6330  0.2939
[ CUDAFloatType{3,2} ]
output_gate 0.4822  0.5038
 0.5010  0.4982
 0.4656  0.5088
[ CUDAFloatType{3,2} ]
candidate_cell 0.3982  0.5905
 0.3767  0.5961
 0.4177  0.5854
[ CUDAFloatType{3,2} ]
X 0.0000  0.0000  0.1542  1.0000
 0.0000  0.0000  0.0023  1.0000
 0.0000  0.0000  0.2896  1.0000
[ CUDAFloatType{3,4} ]
gate_weights.sizes()[3, 4, 2]
gate_weights(1,.,.) =
  0.5017 -0.8094
 -0.0710  0.0153
 -0.4130  0.3659
  0.4267  0.1386

(2,.,.) =
  0.4528 -0.7341
  0.0039 -0.0071
 -0.5034  0.3893
  0.5156  0.0454

(3,.,.) =
  0.5452 -0.8765
 -0.1379  0.0352
 -0.3323  0.3451
  0.3474  0.2217
[ CUDAFloatType{3,4,2} ]
weights.sizes()[8, 4]
weights 0.6010  0.5108  0.3214  0.6770
 0.6453  0.4694 -0.4958  0.2441
-0.2626  0.1918 -0.4936  0.5788
 0.6845  0.4401  0.1471 -0.4400
 0.0896 -0.4237  0.5956 -0.3823
 0.4720  0.2928 -0.1539 -0.2095
 0.2722 -0.1878 -0.5853  0.2427
 0.0263  0.2287  0.6134  0.0776
[ CUDAFloatType{8,4} ]
old_cell 0  0
 0  0
 0  0
[ CUDAFloatType{3,2} ]
weights.grad:
tensor([[-1.2070e-04,  4.5008e-04,  1.9993e-03,  1.0211e-03],
        [-1.2592e-05, -2.9037e-04, -1.4734e-03, -7.6706e-04],
        [-5.9108e-04,  1.2290e-03,  6.1968e-03,  9.4415e-04],
        [-1.9085e-05, -1.2529e-03, -8.8855e-03, -5.2188e-04],
        [ 1.0127e-04, -4.4851e-04, -1.9670e-03, -1.1023e-03],
        [ 1.9397e-05,  3.8178e-04,  2.0509e-03,  7.8026e-04],
        [-5.7299e-06,  1.0336e-04,  4.5438e-04, -7.2841e-05],
        [-4.5990e-05,  2.9184e-04,  1.5033e-03,  7.8825e-04]], device='cuda:0')
bias.grad
tensor([ 0.0035, -0.0025,  0.0085, -0.0105, -0.0035,  0.0033,  0.0009,  0.0023],
       device='cuda:0')
../torch/csrc/utils/python_arg_parser.cpp:698: UserWarning: This overload of addcmul_ is deprecated:
	addcmul_(Number value, Tensor tensor1, Tensor tensor2)
Consider using one of the following signatures instead:
	addcmul_(Tensor tensor1, Tensor tensor2, Number value)
run.py:288: RuntimeWarning: invalid value encountered in double_scalars
  np.sum(epoch_trace) / len(epoch_trace),
epoch 1 finished
	time for this epoch 17.50464677810669
	training_loss =   nan
	validation_loss = 0.2498
total time to train 17.50464677810669
Training ended:
	test loss 0.3352















(base) bash-4.2$ python run.py --timing --verbose --model=subLSTM --cuda --batch-size=4 --seq-length=10 --nhid=5 --epochs=1 --training-size=400
cuda is available: True
cudnn enabled: True
Training subLSTM model with parameters:
	number of layers: 1
	hidden units: 5
	max epochs: 1
	batch size: 4
	optimizer: rmsprop, lr=0.0001, l2=0
	using CUDA
training epoch 1
weights.grad:
tensor([[-1.3028e-02, -7.8852e-03,  1.3004e-02, -2.5968e-02,  3.7322e-03,
          4.7623e-02,  3.9602e-02],
        [-4.7613e-03, -3.1697e-03,  6.0573e-03, -1.1180e-02,  2.8693e-03,
          1.8737e-02,  2.0047e-02],
        [ 3.8093e-03,  2.1536e-03, -2.8880e-03,  6.3758e-03, -2.0764e-04,
         -1.2516e-02, -1.0601e-02],
        [-2.5939e-03, -1.7664e-03,  3.7268e-03, -6.6544e-03,  1.9588e-03,
          1.1184e-02,  1.1541e-02],
        [-6.6789e-03, -4.2367e-03,  7.9347e-03, -1.4993e-02,  3.1947e-03,
          2.6609e-02,  2.3472e-02],
        [-1.5159e-02, -1.1627e-02,  2.8361e-02, -4.7514e-02,  1.8129e-02,
          7.2286e-02,  7.6568e-02],
        [-1.2348e-02, -9.5126e-03,  2.3894e-02, -3.9774e-02,  1.5350e-02,
          5.9854e-02,  5.9304e-02],
        [ 8.6263e-03,  4.9958e-03, -6.7998e-03,  1.4719e-02, -9.5783e-04,
         -2.8730e-02, -2.8266e-02],
        [-5.9387e-03, -4.5894e-03,  1.1716e-02, -1.9390e-02,  7.6346e-03,
          2.8551e-02,  2.8762e-02],
        [-1.0345e-02, -8.2995e-03,  2.2703e-02, -3.6866e-02,  1.5279e-02,
          5.2677e-02,  4.8649e-02],
        [ 1.1738e-02,  7.1682e-03, -1.1648e-02,  2.3315e-02, -3.4582e-03,
         -4.3153e-02, -3.9543e-02],
        [ 5.5444e-03,  3.6957e-03, -7.3006e-03,  1.3337e-02, -3.5234e-03,
         -2.2357e-02, -2.2561e-02],
        [-3.7343e-03, -2.1178e-03,  2.8396e-03, -6.2623e-03,  2.1627e-04,
          1.2442e-02,  1.0595e-02],
        [ 2.3233e-03,  1.5852e-03, -3.3648e-03,  5.9958e-03, -1.7780e-03,
         -1.0181e-02, -1.0451e-02],
        [ 7.1521e-03,  4.5384e-03, -8.3249e-03,  1.5839e-02, -3.3243e-03,
         -2.7953e-02, -2.6430e-02],
        [-4.7941e-03, -2.7303e-03,  3.9690e-03, -8.4650e-03,  6.7630e-04,
          1.5647e-02,  1.4706e-02],
        [-1.3615e-03, -8.5835e-04,  1.5502e-03, -2.9596e-03,  6.2460e-04,
          4.9161e-03,  5.0240e-03],
        [ 1.5099e-04,  9.3409e-05, -9.7591e-05,  2.2753e-04, -1.4062e-06,
         -4.8980e-04, -4.8344e-04],
        [-9.4083e-04, -6.5661e-04,  1.3942e-03, -2.4611e-03,  7.5937e-04,
          3.8962e-03,  4.0381e-03],
        [ 4.1811e-04,  3.6089e-04, -8.1696e-04,  1.3558e-03, -5.4327e-04,
         -2.3450e-03, -2.4152e-03]], device='cuda:0')
bias.grad
tensor([ 0.1816,  0.0722, -0.0478,  0.0416,  0.0996,  0.2786,  0.2322, -0.1084,
         0.1125,  0.2096, -0.1628, -0.0855,  0.0469, -0.0374, -0.1055,  0.0612,
         0.0196, -0.0018,  0.0153, -0.0079], device='cuda:0')
../torch/csrc/utils/python_arg_parser.cpp:698: UserWarning: This overload of addcmul_ is deprecated:
	addcmul_(Number value, Tensor tensor1, Tensor tensor2)
Consider using one of the following signatures instead:
	addcmul_(Tensor tensor1, Tensor tensor2, Number value)
	[batches    10 /    80] loss: 0.69506
	[batches    20 /    80] loss: 0.56578
	[batches    30 /    80] loss: 0.47163
	[batches    40 /    80] loss: 0.40916
	[batches    50 /    80] loss: 0.36570
	[batches    60 /    80] loss: 0.33462
	[batches    70 /    80] loss: 0.31301
	[batches    80 /    80] loss: 0.29913
epoch 1 finished
	time for this epoch 3.711806058883667
	training_loss = 0.4318
	validation_loss = 0.1984
total time to train 3.711806058883667
Training ended:
	test loss 0.2469





















































(base) bash-4.2$ python run.py --timing --verbose --model=subLSTMCuda --cuda --batch-size=4 --seq-length=10 --nhid=5 --epochs=1 --training-size=400
cuda is available: True
cudnn enabled: True
Training subLSTMCuda model with parameters:
	number of layers: 1
	hidden units: 5
	max epochs: 1
	batch size: 4
	optimizer: rmsprop, lr=0.0001, l2=0
	using CUDA
training epoch 1
weights.grad:
tensor([[-7.3554e-03, -4.8021e-03,  9.1275e-03, -1.6932e-02,  4.1361e-03,
          2.9031e-02,  2.9447e-02],
        [-2.9854e-03, -2.2245e-03,  5.0034e-03, -8.5954e-03,  3.0462e-03,
          1.3313e-02,  1.5021e-02],
        [ 2.4324e-03,  1.4162e-03, -1.8736e-03,  4.0773e-03, -2.7589e-04,
         -8.1397e-03, -9.0273e-03],
        [-1.7649e-03, -1.3275e-03,  3.2503e-03, -5.4614e-03,  2.0492e-03,
          8.4531e-03,  8.7092e-03],
        [-3.5559e-03, -2.5452e-03,  5.8879e-03, -1.0144e-02,  3.4243e-03,
          1.5912e-02,  1.5564e-02],
        [-1.4969e-02, -1.1525e-02,  2.8222e-02, -4.7202e-02,  1.8146e-02,
          7.1805e-02,  7.6468e-02],
        [-1.1587e-02, -9.1117e-03,  2.3351e-02, -3.8525e-02,  1.5428e-02,
          5.7510e-02,  5.8886e-02],
        [ 7.2370e-03,  4.2497e-03, -5.7617e-03,  1.2376e-02, -1.0542e-03,
         -2.4214e-02, -2.7569e-02],
        [-5.8892e-03, -4.5641e-03,  1.1679e-02, -1.9308e-02,  7.6371e-03,
          2.8408e-02,  2.8722e-02],
        [-9.8797e-03, -8.0508e-03,  2.2367e-02, -3.6098e-02,  1.5324e-02,
          5.1263e-02,  4.8366e-02],
        [ 6.8258e-03,  4.4954e-03, -8.3276e-03,  1.5539e-02, -3.7980e-03,
         -2.7062e-02, -2.9487e-02],
        [ 3.4742e-03,  2.5875e-03, -6.0536e-03,  1.0300e-02, -3.7222e-03,
         -1.5931e-02, -1.6970e-02],
        [-2.3924e-03, -1.3955e-03,  1.8493e-03, -4.0202e-03,  2.7853e-04,
          8.1104e-03,  9.0201e-03],
        [ 1.5784e-03,  1.1894e-03, -2.9360e-03,  4.9228e-03, -1.8584e-03,
         -7.6923e-03, -7.8762e-03],
        [ 3.8496e-03,  2.7605e-03, -6.1919e-03,  1.0750e-02, -3.5784e-03,
         -1.6876e-02, -1.7496e-02],
        [-2.6370e-03, -1.6416e-03,  2.6730e-03, -5.2779e-03,  9.5319e-04,
          9.1757e-03,  1.0481e-02],
        [-7.6759e-04, -5.5355e-04,  1.2098e-03, -2.1084e-03,  7.0139e-04,
          3.1554e-03,  3.4725e-03],
        [ 9.9290e-05,  6.1257e-05, -6.1887e-05,  1.4423e-04, -3.3746e-06,
         -3.0557e-04, -4.0264e-04],
        [-6.8265e-04, -5.1248e-04,  1.2404e-03, -2.0877e-03,  7.7851e-04,
          3.0782e-03,  3.2283e-03],
        [ 3.6314e-04,  2.9610e-04, -7.3292e-04,  1.2099e-03, -4.9228e-04,
         -1.9709e-03, -2.1869e-03]], device='cuda:0')
bias.grad
tensor([ 0.0550,  0.0258, -0.0150,  0.0161,  0.0307,  0.1381,  0.1111, -0.0450,
         0.0559,  0.1017, -0.0505, -0.0307,  0.0147, -0.0145, -0.0327,  0.0180,
         0.0064, -0.0006,  0.0062, -0.0035], device='cuda:0')
../torch/csrc/utils/python_arg_parser.cpp:698: UserWarning: This overload of addcmul_ is deprecated:
	addcmul_(Number value, Tensor tensor1, Tensor tensor2)
Consider using one of the following signatures instead:
	addcmul_(Tensor tensor1, Tensor tensor2, Number value)
	[batches    10 /    80] loss: 0.70262
	[batches    20 /    80] loss: 0.59005
	[batches    30 /    80] loss: 0.50532
	[batches    40 /    80] loss: 0.44769
	[batches    50 /    80] loss: 0.40644
	[batches    60 /    80] loss: 0.37465
	[batches    70 /    80] loss: 0.34901
	[batches    80 /    80] loss: 0.32813
epoch 1 finished
	time for this epoch 28.722397565841675
	training_loss = 0.4630
	validation_loss = 0.2589
total time to train 28.722397565841675
Training ended:
	test loss 0.3208























































##### printing gradients at time step 2 instead of 1 (i.e. if (i==1))

	(base) bash-4.2$ python run.py --timing --verbose --model=subLSTM --cuda --batch-size=4 --seq-length=10 --nhid=5 --epochs=1 --training-size=400
	cuda is available: True
	cudnn enabled: True
	Training subLSTM model with parameters:
		number of layers: 1
		hidden units: 5
		max epochs: 1
		batch size: 4
		optimizer: rmsprop, lr=0.0001, l2=0
		using CUDA
	training epoch 1
	../torch/csrc/utils/python_arg_parser.cpp:698: UserWarning: This overload of addcmul_ is deprecated:
		addcmul_(Number value, Tensor tensor1, Tensor tensor2)
	Consider using one of the following signatures instead:
		addcmul_(Tensor tensor1, Tensor tensor2, Number value)
	weights.grad:
	tensor([[-1.2630e-02, -7.6322e-03,  1.2776e-02, -2.5542e-02,  3.8233e-03,
	          4.7139e-02,  3.9196e-02],
	        [-4.5988e-03, -3.0647e-03,  5.9668e-03, -1.1014e-02,  2.9037e-03,
	          1.8570e-02,  1.9845e-02],
	        [ 3.6827e-03,  2.0759e-03, -2.8101e-03,  6.2232e-03, -2.2820e-04,
	         -1.2310e-02, -1.0414e-02],
	        [-2.4931e-03, -1.7003e-03,  3.6601e-03, -6.5365e-03,  1.9698e-03,
	          1.1048e-02,  1.1405e-02],
	        [-6.5321e-03, -4.1360e-03,  7.8636e-03, -1.4886e-02,  3.2455e-03,
	          2.6599e-02,  2.3413e-02],
	        [-1.4567e-02, -1.1215e-02,  2.7958e-02, -4.6852e-02,  1.8184e-02,
	          7.1650e-02,  7.5769e-02],
	        [-1.1839e-02, -9.1589e-03,  2.3529e-02, -3.9175e-02,  1.5383e-02,
	          5.9237e-02,  5.8612e-02],
	        [ 8.3275e-03,  4.8105e-03, -6.6028e-03,  1.4340e-02, -9.8832e-04,
	         -2.8235e-02, -2.7810e-02],
	        [-5.6808e-03, -4.4107e-03,  1.1526e-02, -1.9076e-02,  7.6445e-03,
	          2.8233e-02,  2.8421e-02],
	        [-1.0006e-02, -8.0563e-03,  2.2523e-02, -3.6611e-02,  1.5380e-02,
	          5.2622e-02,  4.8551e-02],
	        [ 1.1407e-02,  6.9563e-03, -1.1471e-02,  2.2988e-02, -3.5469e-03,
	         -4.2816e-02, -3.9202e-02],
	        [ 5.3403e-03,  3.5627e-03, -7.1725e-03,  1.3106e-02, -3.5522e-03,
	         -2.2092e-02, -2.2279e-02],
	        [-3.6056e-03, -2.0390e-03,  2.7595e-03, -6.1046e-03,  2.3594e-04,
	          1.2221e-02,  1.0395e-02],
	        [ 2.2335e-03,  1.5263e-03, -3.3057e-03,  5.8915e-03, -1.7881e-03,
	         -1.0059e-02, -1.0331e-02],
	        [ 7.0077e-03,  4.4392e-03, -8.2648e-03,  1.5752e-02, -3.3853e-03,
	         -2.7997e-02, -2.6414e-02],
	        [-4.5861e-03, -2.6039e-03,  3.8247e-03, -8.1811e-03,  6.8979e-04,
	          1.5228e-02,  1.4318e-02],
	        [-1.2932e-03, -8.1482e-04,  1.4951e-03, -2.8575e-03,  6.2023e-04,
	          4.7742e-03,  4.8763e-03],
	        [ 1.5476e-04,  9.5060e-05, -1.0181e-04,  2.3700e-04, -2.9208e-06,
	         -5.1096e-04, -4.9719e-04],
	        [-8.9537e-04, -6.2566e-04,  1.3563e-03, -2.3944e-03,  7.5501e-04,
	          3.8114e-03,  3.9497e-03],
	        [ 4.2485e-04,  3.6371e-04, -8.3314e-04,  1.3895e-03, -5.5584e-04,
	         -2.4240e-03, -2.4870e-03]], device='cuda:0')
	bias.grad
	tensor([ 0.1795,  0.0714, -0.0469,  0.0410,  0.0994,  0.2758,  0.2296, -0.1063,
	         0.1111,  0.2091, -0.1613, -0.0843,  0.0460, -0.0369, -0.1055,  0.0595,
	         0.0191, -0.0018,  0.0149, -0.0082], device='cuda:0')
		[batches    10 /    80] loss: 0.69506
		[batches    20 /    80] loss: 0.56578
		[batches    30 /    80] loss: 0.47163
		[batches    40 /    80] loss: 0.40916
		[batches    50 /    80] loss: 0.36570
		[batches    60 /    80] loss: 0.33462
		[batches    70 /    80] loss: 0.31301
		[batches    80 /    80] loss: 0.29913
	epoch 1 finished
		time for this epoch 0.5483288764953613
		training_loss = 0.4318
		validation_loss = 0.1984
	total time to train 0.5483288764953613
	Training ended:
		test loss 0.2469
























		(base) bash-4.2$ python run.py --timing --verbose --model=subLSTMCuda --cuda --batch-size=4 --seq-length=10 --nhid=5 --epochs=1 --training-size=400
		cuda is available: True
		cudnn enabled: True
		Training subLSTMCuda model with parameters:
			number of layers: 1
			hidden units: 5
			max epochs: 1
			batch size: 4
			optimizer: rmsprop, lr=0.0001, l2=0
			using CUDA
		training epoch 1
		../torch/csrc/utils/python_arg_parser.cpp:698: UserWarning: This overload of addcmul_ is deprecated:
			addcmul_(Number value, Tensor tensor1, Tensor tensor2)
		Consider using one of the following signatures instead:
			addcmul_(Tensor tensor1, Tensor tensor2, Number value)
		weights.grad:
		tensor([[-1.2719e-02, -7.6882e-03,  1.2816e-02, -2.5613e-02,  3.7916e-03,
		          4.7186e-02,  3.9238e-02],
		        [-4.6342e-03, -3.0867e-03,  5.9802e-03, -1.1038e-02,  2.8904e-03,
		          1.8581e-02,  1.9861e-02],
		        [ 3.7041e-03,  2.0893e-03, -2.8188e-03,  6.2380e-03, -2.1902e-04,
		         -1.2315e-02, -1.0421e-02],
		        [-2.5128e-03, -1.7127e-03,  3.6683e-03, -6.5503e-03,  1.9632e-03,
		          1.1054e-02,  1.1412e-02],
		        [-6.5782e-03, -4.1656e-03,  7.8852e-03, -1.4921e-02,  3.2312e-03,
		          2.6616e-02,  2.3438e-02],
		        [-1.4697e-02, -1.1299e-02,  2.8016e-02, -4.6946e-02,  1.8150e-02,
		          7.1692e-02,  7.5833e-02],
		        [-1.1951e-02, -9.2307e-03,  2.3582e-02, -3.9262e-02,  1.5356e-02,
		          5.9288e-02,  5.8669e-02],
		        [ 8.3805e-03,  4.8435e-03, -6.6253e-03,  1.4381e-02, -9.6786e-04,
		         -2.8258e-02, -2.7831e-02],
		        [-5.7313e-03, -4.4431e-03,  1.1547e-02, -1.9109e-02,  7.6288e-03,
		          2.8240e-02,  2.8432e-02],
		        [-1.0106e-02, -8.1213e-03,  2.2572e-02, -3.6689e-02,  1.5358e-02,
		          5.2665e-02,  4.8603e-02],
		        [ 1.1480e-02,  7.0023e-03, -1.1499e-02,  2.3036e-02, -3.5166e-03,
		         -4.2831e-02, -3.9227e-02],
		        [ 5.3855e-03,  3.5910e-03, -7.1935e-03,  1.3143e-02, -3.5393e-03,
		         -2.2122e-02, -2.2310e-02],
		        [-3.6279e-03, -2.0529e-03,  2.7690e-03, -6.1214e-03,  2.2697e-04,
		          1.2231e-02,  1.0405e-02],
		        [ 2.2511e-03,  1.5373e-03, -3.3128e-03,  5.9035e-03, -1.7821e-03,
		         -1.0064e-02, -1.0337e-02],
		        [ 7.0536e-03,  4.4687e-03, -8.2838e-03,  1.5783e-02, -3.3683e-03,
		         -2.8001e-02, -2.6430e-02],
		        [-4.6383e-03, -2.6359e-03,  3.8597e-03, -8.2484e-03,  6.8508e-04,
		          1.5325e-02,  1.4409e-02],
		        [-1.3095e-03, -8.2513e-04,  1.5074e-03, -2.8800e-03,  6.2055e-04,
		          4.8041e-03,  4.9078e-03],
		        [ 1.5296e-04,  9.4143e-05, -1.0016e-04,  2.3316e-04, -2.4484e-06,
		         -5.0240e-04, -4.9055e-04],
		        [-9.0496e-04, -6.3191e-04,  1.3626e-03, -2.4055e-03,  7.5449e-04,
		          3.8233e-03,  3.9629e-03],
		        [ 4.2259e-04,  3.6248e-04, -8.2841e-04,  1.3796e-03, -5.5258e-04,
		         -2.4016e-03, -2.4655e-03]], device='cuda:0')
		bias.grad
		tensor([ 0.1797,  0.0714, -0.0470,  0.0411,  0.0995,  0.2760,  0.2298, -0.1064,
		         0.1112,  0.2093, -0.1614, -0.0845,  0.0461, -0.0370, -0.1055,  0.0599,
		         0.0192, -0.0018,  0.0150, -0.0081], device='cuda:0')
			[batches    10 /    80] loss: 0.70260
			[batches    20 /    80] loss: 0.58991
			[batches    30 /    80] loss: 0.50493
			[batches    40 /    80] loss: 0.44654
			[batches    50 /    80] loss: 0.40462
			[batches    60 /    80] loss: 0.37254
			[batches    70 /    80] loss: 0.34694
			[batches    80 /    80] loss: 0.32632
		epoch 1 finished
			time for this epoch 3.0048599243164062
			training_loss = 0.4618
			validation_loss = 0.2561
		total time to train 3.0048599243164062
		Training ended:
			test loss 0.3176




















~~~~~~~~~~~~~~~~~~~~~~~~~~sublstm
training epoch 10
weights.grad:
tensor([[ 8.6213e-05,  1.9772e-04, -2.6561e-04, -2.0694e-04,  2.2995e-04,
          3.9967e-04,  6.6828e-04],
        [-1.4151e-04, -5.5891e-06, -6.3329e-04, -5.2218e-04,  7.8805e-05,
          1.2855e-03,  1.2588e-03],
        [ 1.1196e-04,  2.3906e-04, -1.8943e-04, -5.0619e-05,  7.3877e-05,
          6.9530e-04, -2.9943e-04],
        [ 1.4134e-04,  2.3558e-04, -9.4684e-05, -7.1540e-05,  1.9205e-04,
          6.9628e-04,  3.3418e-04],
        [ 1.5876e-04,  2.2049e-04,  9.6869e-06,  5.7124e-06,  2.1139e-04,
          2.3416e-04,  3.1238e-04],
        [ 1.1538e-03,  1.7877e-03, -3.2317e-04, -1.6406e-04,  1.2929e-03,
          3.6417e-03, -3.9953e-04],
        [ 1.0417e-03,  1.6236e-03, -4.5020e-04, -4.5277e-04,  1.4330e-03,
          4.0117e-03,  3.5203e-04],
        [ 9.7235e-04,  1.4930e-03,  1.5455e-04,  5.6054e-04,  5.0556e-04,
          9.0365e-04, -3.0665e-03],
        [ 8.5971e-04,  1.2343e-03,  1.5926e-04,  2.6714e-04,  7.3042e-04,
          1.4519e-03, -1.2868e-03],
        [ 9.8393e-04,  1.4755e-03, -3.2597e-05,  1.3519e-04,  9.2597e-04,
          1.9412e-03, -1.3699e-03],
        [-3.3469e-05, -1.4686e-04,  3.8639e-04,  3.3318e-04, -2.2587e-04,
         -7.4792e-04, -8.3408e-04],
        [ 3.7266e-05, -1.1557e-04,  5.1676e-04,  3.9313e-04, -1.4088e-04,
         -8.1958e-04, -1.0605e-03],
        [-1.3071e-04, -2.5034e-04,  1.7032e-04,  6.0872e-05, -1.1595e-04,
         -7.3239e-04,  2.9331e-04],
        [-1.3162e-04, -2.3044e-04,  1.1792e-04,  8.3302e-05, -1.8074e-04,
         -6.6002e-04, -2.9881e-04],
        [-1.2020e-04, -1.8641e-04,  9.0618e-05,  1.0780e-04, -2.3058e-04,
         -5.6809e-04, -6.3022e-04],
        [ 1.4507e-04,  3.1607e-06,  5.5097e-04,  4.2398e-04,  3.2178e-05,
         -9.9966e-05,  1.7780e-04],
        [ 3.3173e-05, -5.9342e-05,  2.3305e-04,  1.5129e-04,  3.8165e-05,
          3.5245e-04,  8.1124e-04],
        [-1.4550e-05, -1.6294e-04,  3.7783e-04,  1.7392e-04,  1.2427e-05,
         -5.2749e-04,  1.7738e-04],
        [ 2.0794e-04,  1.5103e-04,  4.5580e-04,  3.7234e-04,  1.1113e-04,
         -2.1052e-04,  8.8582e-05],
        [ 6.8009e-05, -2.7460e-05,  3.6201e-04,  2.9481e-04, -3.8483e-05,
         -3.6676e-04, -1.8358e-04]], device='cuda:0')
bias.grad
tensor([-1.1731e-03, -3.8128e-04, -1.7904e-05,  2.7396e-05, -8.1677e-04,
         4.7320e-04,  4.6343e-04, -1.1696e-03,  1.7408e-04, -5.5571e-04,
         6.7424e-04,  1.0375e-03, -1.9210e-04, -9.7257e-05,  3.9577e-04,
        -7.2970e-04, -6.2116e-04,  5.5266e-04, -3.9605e-04, -5.9430e-04],
       device='cuda:0')
	[batches    10 /    80] loss: 0.00136
	[batches    20 /    80] loss: 0.00101
	[batches    30 /    80] loss: 0.00073
	[batches    40 /    80] loss: 0.00051
	[batches    50 /    80] loss: 0.00035
	[batches    60 /    80] loss: 0.00023
	[batches    70 /    80] loss: 0.00015
	[batches    80 /    80] loss: 0.00009
epoch 10 finished
	time for this epoch 0.4373140335083008
	training_loss = 0.0006
	validation_loss = 0.7265


























~~~~~~~~~~~~~~~~~~~~~~ trainign subLSTMCuda
training epoch 10
weights.grad:
tensor([[ 1.2329e-04,  1.0035e-04, -2.4464e-04, -3.0245e-04,  2.1499e-04,
          7.4913e-05,  9.1980e-04],
        [-1.7214e-04, -1.4606e-04, -6.8068e-04, -6.0023e-04, -8.2411e-05,
          1.0862e-03,  1.3955e-03],
        [ 1.3702e-04,  2.0425e-04, -2.3380e-05, -1.0191e-04,  1.3952e-04,
          8.0010e-04, -1.6469e-04],
        [ 1.8388e-04,  2.6609e-04,  2.1843e-06, -9.6179e-05,  1.6507e-04,
          5.3834e-04, -1.2396e-04],
        [ 2.0088e-04,  2.1721e-04,  6.7091e-05, -7.0039e-05,  2.3080e-04,
          2.6201e-04,  1.7670e-04],
        [ 1.2455e-03,  1.7404e-03,  1.1793e-04, -8.3060e-04,  1.2830e-03,
          3.3976e-03, -1.2442e-03],
        [ 1.0255e-03,  1.4006e-03,  1.1995e-04, -1.3347e-03,  1.4576e-03,
          4.0552e-03,  1.3322e-04],
        [ 8.8291e-04,  1.3505e-03,  7.9238e-05,  1.3016e-04,  4.3445e-04,
          1.3141e-03, -2.9419e-03],
        [ 8.9779e-04,  1.1761e-03,  5.7237e-04, -1.6417e-07,  7.8608e-04,
          8.4588e-04, -2.2852e-03],
        [ 1.1818e-03,  1.6185e-03,  4.5733e-04, -2.0041e-04,  1.0328e-03,
          1.5758e-03, -2.7952e-03],
        [-2.4287e-05,  8.3235e-06,  3.6643e-04,  3.8543e-04, -1.2771e-04,
         -4.0085e-04, -1.1592e-03],
        [ 4.9424e-05,  2.1895e-05,  5.8090e-04,  5.5588e-04, -3.9078e-05,
         -5.9580e-04, -1.1059e-03],
        [-1.5825e-04, -2.0383e-04, -4.0003e-05,  1.3638e-04, -2.1012e-04,
         -7.9382e-04,  1.2584e-04],
        [-1.7319e-04, -2.5716e-04,  4.0278e-06,  1.1979e-04, -1.6694e-04,
         -5.3170e-04,  1.0563e-04],
        [-1.6998e-04, -1.9604e-04, -2.3901e-06,  2.0826e-04, -2.5240e-04,
         -5.8465e-04, -4.3658e-04],
        [ 1.7498e-04,  2.0056e-05,  4.2781e-04,  4.8752e-04,  8.1058e-05,
         -1.3195e-04,  4.8727e-04],
        [-1.4131e-06, -8.1986e-05,  1.5255e-04,  1.3529e-04,  7.8240e-06,
          3.8666e-04,  7.8937e-04],
        [ 1.6184e-04,  1.2602e-06,  6.9030e-04,  5.0497e-04,  1.4844e-04,
         -8.5493e-04,  2.4791e-04],
        [ 1.4202e-04,  1.1493e-04,  2.5172e-04,  2.6537e-04,  6.5860e-05,
         -2.2262e-04, -5.1244e-05],
        [ 5.4662e-05, -5.3734e-05,  2.5058e-04,  3.7896e-04, -4.6276e-05,
         -4.7281e-04, -4.4543e-05]], device='cuda:0')
bias.grad
tensor([-2.7201e-03, -1.6680e-03, -2.6389e-04, -2.7012e-04, -9.5486e-04,
        -1.5681e-03, -7.5290e-04, -1.5707e-03,  1.9986e-04, -7.5259e-04,
         2.0883e-03,  2.2602e-03, -1.3287e-04,  3.6683e-05,  6.0599e-04,
        -1.1926e-03, -5.1821e-04,  5.4054e-04, -4.2739e-04, -5.2928e-04],
       device='cuda:0')
	[batches    10 /    80] loss: 0.00192
	[batches    20 /    80] loss: 0.00150
	[batches    30 /    80] loss: 0.00116
	[batches    40 /    80] loss: 0.00088
	[batches    50 /    80] loss: 0.00065
	[batches    60 /    80] loss: 0.00048
	[batches    70 /    80] loss: 0.00034
	[batches    80 /    80] loss: 0.00024
epoch 10 finished
	time for this epoch 3.942711353302002
	training_loss = 0.0009
	validation_loss = 0.6227


80 without contiguouses
64 with
72 with





















(base) bash-4.2$ python run.py --timing --verbose --model=subLSTM --cuda --batch-size=4 --seq-length=10 --nhid=5 --epochs=2 --training-size=400
cuda is available: True
cudnn enabled: True
Training subLSTM model with parameters:
	number of layers: 1
	hidden units: 5
	max epochs: 2
	batch size: 4
	optimizer: rmsprop, lr=0.0001, l2=0
	using CUDA
training epoch 1
weights.grad:
tensor([[-1.3028e-02, -7.8852e-03,  1.3004e-02, -2.5968e-02,  3.7322e-03,
          4.7623e-02,  3.9602e-02],
        [-4.7613e-03, -3.1697e-03,  6.0573e-03, -1.1180e-02,  2.8693e-03,
          1.8737e-02,  2.0047e-02],
        [ 3.8093e-03,  2.1536e-03, -2.8880e-03,  6.3758e-03, -2.0764e-04,
         -1.2516e-02, -1.0601e-02],
        [-2.5939e-03, -1.7664e-03,  3.7268e-03, -6.6544e-03,  1.9588e-03,
          1.1184e-02,  1.1541e-02],
        [-6.6789e-03, -4.2367e-03,  7.9347e-03, -1.4993e-02,  3.1947e-03,
          2.6609e-02,  2.3472e-02],
        [-1.5159e-02, -1.1627e-02,  2.8361e-02, -4.7514e-02,  1.8129e-02,
          7.2286e-02,  7.6568e-02],
        [-1.2348e-02, -9.5126e-03,  2.3894e-02, -3.9774e-02,  1.5350e-02,
          5.9854e-02,  5.9304e-02],
        [ 8.6263e-03,  4.9958e-03, -6.7998e-03,  1.4719e-02, -9.5783e-04,
         -2.8730e-02, -2.8266e-02],
        [-5.9387e-03, -4.5894e-03,  1.1716e-02, -1.9390e-02,  7.6346e-03,
          2.8551e-02,  2.8762e-02],
        [-1.0345e-02, -8.2995e-03,  2.2703e-02, -3.6866e-02,  1.5279e-02,
          5.2677e-02,  4.8649e-02],
        [ 1.1738e-02,  7.1682e-03, -1.1648e-02,  2.3315e-02, -3.4582e-03,
         -4.3153e-02, -3.9543e-02],
        [ 5.5444e-03,  3.6957e-03, -7.3006e-03,  1.3337e-02, -3.5234e-03,
         -2.2357e-02, -2.2561e-02],
        [-3.7343e-03, -2.1178e-03,  2.8396e-03, -6.2623e-03,  2.1627e-04,
          1.2442e-02,  1.0595e-02],
        [ 2.3233e-03,  1.5852e-03, -3.3648e-03,  5.9958e-03, -1.7780e-03,
         -1.0181e-02, -1.0451e-02],
        [ 7.1521e-03,  4.5384e-03, -8.3249e-03,  1.5839e-02, -3.3243e-03,
         -2.7953e-02, -2.6430e-02],
        [-4.7941e-03, -2.7303e-03,  3.9690e-03, -8.4650e-03,  6.7630e-04,
          1.5647e-02,  1.4706e-02],
        [-1.3615e-03, -8.5835e-04,  1.5502e-03, -2.9596e-03,  6.2460e-04,
          4.9161e-03,  5.0240e-03],
        [ 1.5099e-04,  9.3409e-05, -9.7591e-05,  2.2753e-04, -1.4062e-06,
         -4.8980e-04, -4.8344e-04],
        [-9.4083e-04, -6.5661e-04,  1.3942e-03, -2.4611e-03,  7.5937e-04,
          3.8962e-03,  4.0381e-03],
        [ 4.1811e-04,  3.6089e-04, -8.1696e-04,  1.3558e-03, -5.4327e-04,
         -2.3450e-03, -2.4152e-03]], device='cuda:0')
bias.grad
tensor([ 0.1816,  0.0722, -0.0478,  0.0416,  0.0996,  0.2786,  0.2322, -0.1084,
         0.1125,  0.2096, -0.1628, -0.0855,  0.0469, -0.0374, -0.1055,  0.0612,
         0.0196, -0.0018,  0.0153, -0.0079], device='cuda:0')
../torch/csrc/utils/python_arg_parser.cpp:698: UserWarning: This overload of addcmul_ is deprecated:
	addcmul_(Number value, Tensor tensor1, Tensor tensor2)
Consider using one of the following signatures instead:
	addcmul_(Tensor tensor1, Tensor tensor2, Number value)
	[batches    10 /    80] loss: 0.69506
	[batches    20 /    80] loss: 0.56578
	[batches    30 /    80] loss: 0.47163
	[batches    40 /    80] loss: 0.40916
	[batches    50 /    80] loss: 0.36570
	[batches    60 /    80] loss: 0.33462
	[batches    70 /    80] loss: 0.31301
	[batches    80 /    80] loss: 0.29913
epoch 1 finished
	time for this epoch 0.5445504188537598
	training_loss = 0.4318
	validation_loss = 0.1984
training epoch 2
weights.grad:
tensor([[ 2.0470e-03,  2.2268e-03,  9.1606e-04, -1.1118e-03,  2.3278e-03,
          1.3308e-02,  6.3898e-03],
        [ 1.6167e-03,  1.7325e-03,  6.8074e-04, -7.0725e-04,  1.6904e-03,
          7.6909e-03,  3.5719e-03],
        [ 4.2248e-04,  4.1353e-04,  1.8200e-04, -1.7591e-04,  4.4134e-04,
          1.5133e-03,  6.2746e-04],
        [ 2.9112e-04,  2.8942e-04,  1.0412e-04, -7.0870e-05,  2.5321e-04,
          3.2242e-04,  1.3050e-04],
        [ 1.9095e-03,  2.0803e-03,  8.5945e-04, -1.0658e-03,  2.2081e-03,
          1.1858e-02,  5.3551e-03],
        [ 7.0364e-03,  7.7218e-03,  2.7943e-03, -2.6515e-03,  6.8596e-03,
          2.4978e-02,  8.5884e-03],
        [ 5.7684e-03,  6.2723e-03,  2.3439e-03, -2.2761e-03,  5.7642e-03,
          2.0215e-02,  6.7334e-03],
        [ 1.0420e-03,  1.1404e-03,  3.9261e-04, -3.4093e-04,  9.6484e-04,
          1.4076e-03, -1.1631e-04],
        [ 1.0878e-03,  1.1074e-03,  4.1040e-04, -2.9940e-04,  9.8524e-04,
         -6.4332e-05, -5.0827e-04],
        [ 7.9536e-03,  8.6550e-03,  3.1304e-03, -2.9829e-03,  7.7235e-03,
          2.3535e-02,  8.7856e-03],
        [-2.2657e-03, -2.5948e-03, -9.8852e-04,  1.2007e-03, -2.5138e-03,
         -1.5514e-02, -6.2397e-03],
        [-1.2988e-03, -1.5085e-03, -5.0267e-04,  5.1179e-04, -1.2576e-03,
         -6.4225e-03, -2.5934e-03],
        [-4.1142e-04, -3.9384e-04, -1.8074e-04,  1.7644e-04, -4.3831e-04,
         -1.4506e-03, -6.4652e-04],
        [-2.8352e-04, -2.8052e-04, -1.0315e-04,  7.4498e-05, -2.5236e-04,
         -2.9166e-04, -1.7651e-04],
        [-2.7848e-03, -2.9202e-03, -1.2992e-03,  1.5964e-03, -3.3074e-03,
         -1.7508e-02, -8.3889e-03],
        [-8.6144e-04, -1.0775e-03, -2.9842e-04,  3.2002e-04, -7.7920e-04,
         -5.7748e-03, -2.7060e-03],
        [-1.7112e-04, -2.4692e-04, -3.9024e-05,  3.8130e-05, -1.1413e-04,
         -6.0694e-04, -5.2871e-04],
        [ 7.7360e-05,  7.7932e-05,  3.2208e-05, -3.1928e-05,  7.9476e-05,
          2.8371e-04,  1.5892e-04],
        [-3.9155e-05,  2.3753e-05, -4.3578e-05,  4.5806e-05, -9.3629e-05,
         -4.3147e-05, -4.3325e-05],
        [-1.2592e-03, -1.4977e-03, -4.9749e-04,  5.9768e-04, -1.2928e-03,
         -1.0408e-02, -5.2911e-03]], device='cuda:0')
bias.grad
tensor([ 1.9746e-02,  8.9213e-03,  2.1692e-03, -9.9174e-05,  1.8649e-02,
         2.9496e-02,  2.5731e-02,  2.8837e-03,  2.2018e-04,  3.5941e-02,
        -2.0493e-02, -6.7048e-03, -2.1724e-03,  1.6490e-05, -2.6345e-02,
        -5.0722e-03, -7.1598e-04,  4.2049e-04, -5.5800e-04, -1.0686e-02],
       device='cuda:0')
	[batches    10 /    80] loss: 0.29117
	[batches    20 /    80] loss: 0.28703
	[batches    30 /    80] loss: 0.28474
	[batches    40 /    80] loss: 0.28298
	[batches    50 /    80] loss: 0.28122
	[batches    60 /    80] loss: 0.27935
	[batches    70 /    80] loss: 0.27739
	[batches    80 /    80] loss: 0.27534
epoch 2 finished
	time for this epoch 0.46660304069519043
	training_loss = 0.2824
	validation_loss = 0.1683
total time to train 1.0111534595489502
Training ended:
	test loss 0.2077



















	(base) bash-4.2$ python run.py --timing --verbose --model=subLSTMCuda --cuda --batch-size=4 --seq-length=10 --nhid=5 --epochs=2 --training-size=400
	cuda is available: True
	cudnn enabled: True
	Training subLSTMCuda model with parameters:
		number of layers: 1
		hidden units: 5
		max epochs: 2
		batch size: 4
		optimizer: rmsprop, lr=0.0001, l2=0
		using CUDA
	training epoch 1
	weights.grad:
	tensor([[-1.3028e-02, -7.8852e-03,  1.3004e-02, -2.5968e-02,  3.7322e-03,
	          4.7623e-02,  3.9602e-02],
	        [-4.7613e-03, -3.1697e-03,  6.0573e-03, -1.1180e-02,  2.8693e-03,
	          1.8737e-02,  2.0047e-02],
	        [ 3.8093e-03,  2.1536e-03, -2.8880e-03,  6.3758e-03, -2.0764e-04,
	         -1.2516e-02, -1.0601e-02],
	        [-2.5939e-03, -1.7664e-03,  3.7268e-03, -6.6544e-03,  1.9588e-03,
	          1.1184e-02,  1.1541e-02],
	        [-6.6789e-03, -4.2367e-03,  7.9347e-03, -1.4993e-02,  3.1947e-03,
	          2.6609e-02,  2.3472e-02],
	        [-1.5159e-02, -1.1627e-02,  2.8361e-02, -4.7514e-02,  1.8129e-02,
	          7.2286e-02,  7.6568e-02],
	        [-1.2348e-02, -9.5126e-03,  2.3894e-02, -3.9774e-02,  1.5350e-02,
	          5.9854e-02,  5.9304e-02],
	        [ 8.6263e-03,  4.9958e-03, -6.7998e-03,  1.4719e-02, -9.5783e-04,
	         -2.8730e-02, -2.8266e-02],
	        [-5.9387e-03, -4.5894e-03,  1.1716e-02, -1.9390e-02,  7.6346e-03,
	          2.8551e-02,  2.8762e-02],
	        [-1.0345e-02, -8.2995e-03,  2.2703e-02, -3.6866e-02,  1.5279e-02,
	          5.2677e-02,  4.8649e-02],
	        [ 1.1738e-02,  7.1682e-03, -1.1648e-02,  2.3315e-02, -3.4582e-03,
	         -4.3153e-02, -3.9543e-02],
	        [ 5.5444e-03,  3.6957e-03, -7.3006e-03,  1.3337e-02, -3.5234e-03,
	         -2.2357e-02, -2.2561e-02],
	        [-3.7343e-03, -2.1178e-03,  2.8396e-03, -6.2623e-03,  2.1627e-04,
	          1.2442e-02,  1.0595e-02],
	        [ 2.3233e-03,  1.5852e-03, -3.3648e-03,  5.9958e-03, -1.7780e-03,
	         -1.0181e-02, -1.0451e-02],
	        [ 7.1521e-03,  4.5384e-03, -8.3249e-03,  1.5839e-02, -3.3243e-03,
	         -2.7953e-02, -2.6430e-02],
	        [-4.7941e-03, -2.7303e-03,  3.9690e-03, -8.4650e-03,  6.7630e-04,
	          1.5647e-02,  1.4706e-02],
	        [-1.3615e-03, -8.5835e-04,  1.5502e-03, -2.9596e-03,  6.2460e-04,
	          4.9161e-03,  5.0240e-03],
	        [ 1.5099e-04,  9.3409e-05, -9.7591e-05,  2.2753e-04, -1.4062e-06,
	         -4.8980e-04, -4.8344e-04],
	        [-9.4083e-04, -6.5661e-04,  1.3942e-03, -2.4611e-03,  7.5937e-04,
	          3.8962e-03,  4.0381e-03],
	        [ 4.1811e-04,  3.6089e-04, -8.1696e-04,  1.3558e-03, -5.4327e-04,
	         -2.3450e-03, -2.4153e-03]], device='cuda:0')
	bias.grad
	tensor([ 0.0908,  0.0361, -0.0239,  0.0208,  0.0498,  0.1393,  0.1161, -0.0542,
	         0.0563,  0.1048, -0.0814, -0.0427,  0.0235, -0.0187, -0.0527,  0.0306,
	         0.0098, -0.0009,  0.0076, -0.0039], device='cuda:0')
	../torch/csrc/utils/python_arg_parser.cpp:698: UserWarning: This overload of addcmul_ is deprecated:
		addcmul_(Number value, Tensor tensor1, Tensor tensor2)
	Consider using one of the following signatures instead:
		addcmul_(Tensor tensor1, Tensor tensor2, Number value)
		[batches    10 /    80] loss: 0.70261
		[batches    20 /    80] loss: 0.59001
		[batches    30 /    80] loss: 0.50521
		[batches    40 /    80] loss: 0.44754
		[batches    50 /    80] loss: 0.40640
		[batches    60 /    80] loss: 0.37482
		[batches    70 /    80] loss: 0.34943
		[batches    80 /    80] loss: 0.32879
	epoch 1 finished
		time for this epoch 3.9308815002441406
		training_loss = 0.4631
		validation_loss = 0.2597
	training epoch 2
	weights.grad:
	tensor([[ 2.3189e-03,  2.2593e-03,  2.9695e-03, -3.9271e-03,  4.4916e-03,
	          1.8383e-02,  1.1288e-02],
	        [ 1.7106e-03,  1.5441e-03,  1.6879e-03, -2.0411e-03,  2.8586e-03,
	          9.0687e-03,  5.9206e-03],
	        [ 5.7886e-04,  5.0664e-04,  5.5908e-04, -6.6259e-04,  9.4810e-04,
	          2.7977e-03,  1.8160e-03],
	        [ 2.9722e-04,  2.2563e-04,  1.0649e-04, -3.6269e-05,  3.0920e-04,
	         -2.0771e-04,  5.2761e-05],
	        [ 2.4235e-03,  2.4227e-03,  3.1756e-03, -4.2648e-03,  4.7780e-03,
	          1.9373e-02,  1.0958e-02],
	        [ 8.5945e-03,  7.6553e-03,  7.4377e-03, -8.4233e-03,  1.3220e-02,
	          3.1991e-02,  1.9378e-02],
	        [ 6.6203e-03,  5.9127e-03,  5.9050e-03, -6.7989e-03,  1.0403e-02,
	          2.5172e-02,  1.4712e-02],
	        [ 1.6978e-03,  1.5262e-03,  1.3188e-03, -1.4264e-03,  2.4517e-03,
	          4.2060e-03,  1.9954e-03],
	        [ 1.1051e-03,  8.7807e-04,  4.6980e-04, -2.6396e-04,  1.2417e-03,
	         -1.3717e-03, -9.2980e-04],
	        [ 1.0322e-02,  9.3224e-03,  8.9974e-03, -1.0231e-02,  1.5828e-02,
	          3.6044e-02,  2.0861e-02],
	        [-2.5680e-03, -2.4965e-03, -3.2148e-03,  4.2343e-03, -4.9241e-03,
	         -2.0344e-02, -1.2192e-02],
	        [-1.5997e-03, -1.4944e-03, -1.5479e-03,  1.8561e-03, -2.5989e-03,
	         -8.2778e-03, -5.0817e-03],
	        [-5.6699e-04, -4.9100e-04, -5.5123e-04,  6.5502e-04, -9.3548e-04,
	         -2.7247e-03, -1.8129e-03],
	        [-2.7910e-04, -2.1414e-04, -1.0090e-04,  3.8484e-05, -2.9424e-04,
	          2.1486e-04, -6.4914e-05],
	        [-3.1610e-03, -3.0834e-03, -4.1531e-03,  5.5929e-03, -6.3211e-03,
	         -2.5960e-02, -1.5866e-02],
	        [-6.4119e-04, -6.0892e-04, -5.3079e-04,  5.9616e-04, -9.0937e-04,
	         -2.8721e-03, -1.8558e-03],
	        [-9.8584e-05, -1.0557e-04, -6.8678e-05,  7.5535e-05, -1.2675e-04,
	         -1.2421e-04, -2.9487e-04],
	        [ 5.7361e-05,  5.5469e-05,  5.4518e-05, -6.6066e-05,  9.2183e-05,
	          3.0523e-04,  2.4219e-04],
	        [ 2.7593e-05,  4.7908e-05, -1.7837e-05,  3.3941e-05, -9.2110e-06,
	         -7.4877e-05, -1.8356e-05],
	        [-1.2925e-03, -1.3152e-03, -1.4948e-03,  1.9395e-03, -2.3077e-03,
	         -1.0661e-02, -6.9352e-03]], device='cuda:0')
	bias.grad
	tensor([ 0.0223,  0.0100,  0.0032, -0.0005,  0.0245,  0.0379,  0.0312,  0.0059,
	        -0.0011,  0.0470, -0.0237, -0.0093, -0.0032,  0.0005, -0.0316, -0.0030,
	        -0.0003,  0.0003, -0.0002, -0.0109], device='cuda:0')
		[batches    10 /    80] loss: 0.31228
		[batches    20 /    80] loss: 0.29957
		[batches    30 /    80] loss: 0.29031
		[batches    40 /    80] loss: 0.28397
		[batches    50 /    80] loss: 0.27980
		[batches    60 /    80] loss: 0.27696
		[batches    70 /    80] loss: 0.27473
		[batches    80 /    80] loss: 0.27267
	epoch 2 finished
		time for this epoch 3.526346206665039
		training_loss = 0.2863
		validation_loss = 0.1744
	total time to train 7.45722770690918
	Training ended:
		test loss 0.2153












		[-2.8343e-03,  1.0753e-03, -8.0877e-04, -1.4385e-03,  8.0549e-04,
		          4.3891e-03,  5.6583e-03],
		        [-1.0031e-02,  8.4087e-04, -1.1247e-02, -6.2265e-03,  3.4807e-03,
		          2.6126e-02,  2.0482e-02],
		        [-1.6492e-02,  2.0107e-03, -1.7771e-02, -9.9206e-03,  5.6957e-03,
		          4.3433e-02,  4.4278e-02],
		        [ 7.5997e-03, -2.2921e-03,  4.0277e-03,  4.0520e-03, -2.2881e-03,
		         -1.4087e-02, -1.7261e-02],
		        [-6.2666e-03,  1.9619e-03, -3.2933e-03, -3.3482e-03,  1.8882e-03,
		          1.1169e-02,  1.3640e-02],
		        [-1.1415e-02,  4.9186e-03, -1.0527e-03, -5.5616e-03,  3.0943e-03,
		          1.4405e-02,  1.5377e-02],
		        [-2.4880e-02,  7.5288e-03, -1.1054e-02, -1.3300e-02,  7.3194e-03,
		          4.3323e-02,  4.0364e-02],
		        [-5.0546e-02,  1.4530e-02, -3.0077e-02, -2.7223e-02,  1.5786e-02,
		          9.6635e-02,  1.0955e-01],
		        [ 2.2703e-02, -9.7636e-03,  1.0406e-03,  1.0913e-02, -6.0749e-03,
		         -2.5874e-02, -2.4709e-02],
		        [-2.2583e-02,  9.0471e-03, -3.6242e-03, -1.1183e-02,  6.2354e-03,
		          3.0284e-02,  3.1550e-02],
		        [ 2.7309e-03, -1.0541e-03,  7.9199e-04,  1.3864e-03, -7.7470e-04,
		         -4.0743e-03, -5.2245e-03],
		        [ 1.0164e-02, -9.3149e-04,  1.1353e-02,  6.2474e-03, -3.5538e-03,
		         -2.6271e-02, -2.2528e-02],
		        [ 1.3749e-02, -1.7003e-03,  1.4249e-02,  8.3558e-03, -4.6290e-03,
		         -3.3204e-02, -2.9722e-02],
		        [-7.4880e-03,  2.2689e-03, -3.9835e-03, -3.9862e-03,  2.2575e-03,
		          1.3767e-02,  1.6983e-02],
		        [ 6.4877e-03, -2.0175e-03,  3.4896e-03,  3.4562e-03, -1.9716e-03,
		         -1.1761e-02, -1.4687e-02],
		        [-1.1271e-03,  4.1416e-04, -3.0418e-04, -5.9379e-04,  3.0927e-04,
		          1.7527e-03,  1.6854e-03],
		        [-8.9592e-04, -1.9586e-04, -1.6175e-03, -6.4473e-04,  3.7106e-04,
		          2.7871e-03,  2.5948e-03],
		        [-1.1465e-02,  1.1521e-03, -1.3131e-02, -7.0004e-03,  4.0079e-03,
		          3.0468e-02,  3.0858e-02],
		        [ 2.1795e-04, -5.9402e-05, -1.6865e-04,  1.5066e-04, -2.2770e-05,
		         -3.8330e-04,  4.9186e-04],
		        [-1.8590e-03,  5.6864e-04, -5.0481e-04, -1.0371e-03,  4.9189e-04,
		          3.3275e-03,  2.5519e-03]], device='cuda:0')






cuda

		bias.grad
		tensor([ 0.0090,  0.0572,  0.0906, -0.0294,  0.0241,  0.0304,  0.0923,  0.2060,
		        -0.0581,  0.0648, -0.0087, -0.0576, -0.0744,  0.0289, -0.0252,  0.0036,
		         0.0068,  0.0649, -0.0002,  0.0061], device='cuda:0')





















				 (base) bash-4.2$ python run.py --timing --verbose --model=subLSTM --cuda --batch-size=4 --seq-length=10 --nhid=5 --epochs=1 --training-size=400
				 cuda is available: True
				 cudnn enabled: True
				 Training subLSTM model with parameters:
				 	number of layers: 1
				 	hidden units: 5
				 	max epochs: 1
				 	batch size: 4
				 	optimizer: rmsprop, lr=0.0001, l2=0
				 	using CUDA
				 training epoch 1
				 ../torch/csrc/utils/python_arg_parser.cpp:698: UserWarning: This overload of addcmul_ is deprecated:
				 	addcmul_(Number value, Tensor tensor1, Tensor tensor2)
				 Consider using one of the following signatures instead:
				 	addcmul_(Tensor tensor1, Tensor tensor2, Number value)
				 weights.grad:
				 tensor([[-2.7513e-03,  1.0778e-03, -7.4003e-04, -1.4158e-03,  7.9893e-04,
				           4.2449e-03,  5.4977e-03],
				         [-9.9985e-03,  9.5550e-04, -1.1185e-02, -6.3407e-03,  3.5777e-03,
				           2.6236e-02,  2.0589e-02],
				         [-1.6230e-02,  2.1734e-03, -1.7422e-02, -9.9693e-03,  5.7749e-03,
				           4.3045e-02,  4.3880e-02],
				         [ 7.6345e-03, -2.3580e-03,  4.0680e-03,  4.1541e-03, -2.3650e-03,
				          -1.4333e-02, -1.7477e-02],
				         [-6.3067e-03,  2.0210e-03, -3.3277e-03, -3.4371e-03,  1.9542e-03,
				           1.1381e-02,  1.3844e-02],
				         [-1.1098e-02,  4.8996e-03, -8.6528e-04, -5.4804e-03,  3.0706e-03,
				           1.3953e-02,  1.4897e-02],
				         [-2.4888e-02,  7.7769e-03, -1.0925e-02, -1.3542e-02,  7.5178e-03,
				           4.3618e-02,  4.0687e-02],
				         [-4.9769e-02,  1.4825e-02, -2.9303e-02, -2.7303e-02,  1.5951e-02,
				           9.5646e-02,  1.0829e-01],
				         [ 2.2741e-02, -9.9333e-03,  1.0381e-03,  1.1117e-02, -6.2360e-03,
				          -2.6222e-02, -2.5091e-02],
				         [-2.2669e-02,  9.2307e-03, -3.6729e-03, -1.1428e-02,  6.4215e-03,
				           3.0791e-02,  3.2095e-02],
				         [ 2.6517e-03, -1.0562e-03,  7.2633e-04,  1.3650e-03, -7.6872e-04,
				          -3.9437e-03, -5.0822e-03],
				         [ 1.0148e-02, -1.0493e-03,  1.1310e-02,  6.3719e-03, -3.6577e-03,
				          -2.6437e-02, -2.2703e-02],
				         [ 1.3551e-02, -1.8387e-03,  1.3985e-02,  8.4064e-03, -4.7031e-03,
				          -3.2970e-02, -2.9544e-02],
				         [-7.5195e-03,  2.3332e-03, -4.0195e-03, -4.0854e-03,  2.3319e-03,
				           1.3999e-02,  1.7174e-02],
				         [ 6.5260e-03, -2.0781e-03,  3.5232e-03,  3.5468e-03, -2.0391e-03,
				          -1.1976e-02, -1.4894e-02],
				         [-1.0858e-03,  4.1085e-04, -2.7994e-04, -5.8078e-04,  3.0501e-04,
				           1.6892e-03,  1.6238e-03],
				         [-8.5428e-04, -1.8883e-04, -1.5796e-03, -6.3403e-04,  3.6988e-04,
				           2.7129e-03,  2.5497e-03],
				         [-1.1198e-02,  1.2587e-03, -1.2798e-02, -6.9849e-03,  4.0369e-03,
				           2.9984e-02,  3.0386e-02],
				         [ 2.3835e-04, -6.7785e-05, -1.6474e-04,  1.6310e-04, -2.9555e-05,
				          -4.1575e-04,  4.5838e-04],
				         [-1.8431e-03,  5.7824e-04, -4.9108e-04, -1.0461e-03,  5.0099e-04,
				           3.3222e-03,  2.5330e-03]], device='cuda:0')
				 bias.grad
				 tensor([ 0.0087,  0.0574,  0.0897, -0.0298,  0.0245,  0.0295,  0.0928,  0.2039,
				         -0.0588,  0.0658, -0.0084, -0.0579, -0.0738,  0.0294, -0.0256,  0.0035,
				          0.0066,  0.0638, -0.0003,  0.0061], device='cuda:0')
				 	[batches    10 /    80] loss: 1.42893
				 	[batches    20 /    80] loss: 1.26779
				 	[batches    30 /    80] loss: 1.11055
				 	[batches    40 /    80] loss: 0.97275
				 	[batches    50 /    80] loss: 0.84891
				 	[batches    60 /    80] loss: 0.73383
				 	[batches    70 /    80] loss: 0.62494
				 	[batches    80 /    80] loss: 0.52218
				 epoch 1 finished
				 	time for this epoch 0.4916801452636719
				 	training_loss = 0.9387
				 	validation_loss = 0.5200
				 total time to train 0.4916801452636719
				 Training ended:
				 	test loss 0.6144
				 (base) bash-4.2$
